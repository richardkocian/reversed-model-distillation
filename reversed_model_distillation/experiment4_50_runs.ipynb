{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "epochs = 15\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "class TeacherModel(nn.Module):\n",
    "    def __init__(self, input_size=32):\n",
    "        super(TeacherModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        fc_input_dim = (input_size // 2) * (input_size // 2) * 16\n",
    "        \n",
    "        self.fc1 = nn.Linear(fc_input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 10) # 10 classes for CIFAR-10\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc2(x)\n",
    "\n",
    "def train_teacher(model, optimizer, criterion, alpha=0.6):\n",
    "    model.train()\n",
    "    training_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, (inputs, targets) in enumerate(train_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss_soft = F.kl_div(F.log_softmax(outputs, dim=1),\n",
    "                                     F.softmax(outputs.detach(), dim=1),\n",
    "                                     reduction='batchmean')\n",
    "\n",
    "            loss_hard = criterion(outputs, targets)\n",
    "\n",
    "            # Combined loss\n",
    "            loss = alpha * loss_hard + (1 - alpha) * loss_soft\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            if i % 100 == 99:  # Print every 100 mini-batches\n",
    "                #print(f'Epoch [{epoch+1}/{epochs}], Batch [{i+1}/{len(train_loader)}], Loss: {running_loss / 100:.4f}')\n",
    "                training_losses.append(running_loss / 100)\n",
    "                running_loss = 0.0\n",
    "    # Plotting training loss over time\n",
    "    plt.plot(training_losses)\n",
    "    plt.xlabel('Batch (100 intervals)')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss during training without distillation')\n",
    "    plt.show()\n",
    "    return training_losses\n",
    "\n",
    "def test_model(model):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    print(f'Accuracy: {accuracy}%')\n",
    "    return accuracy\n",
    "\n",
    "teacher_model = TeacherModel().to(device)\n",
    "teacher_optimizer = optim.Adam(teacher_model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"Training Teacher Model...\")\n",
    "train_teacher(teacher_model, teacher_optimizer, criterion)\n",
    "print(\"Teacher model testing:\")\n",
    "test_model(teacher_model)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class StudentModel(nn.Module):\n",
    "    def __init__(self, input_size=32):  # input_size=32 pro CIFAR-10\n",
    "        super(StudentModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "        fc_input_dim = (input_size // 4) * (input_size // 4) * 256\n",
    "\n",
    "        self.fc1 = nn.Linear(fc_input_dim, 512)\n",
    "        self.fc2 = nn.Linear(512, 10)  # 10 classes for CIFAR-10\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc2(x)\n",
    "\n",
    "def train_student(student_model, teacher_model, optimizer, switch_epoch, alpha=0.6):\n",
    "    student_model.train()\n",
    "    teacher_model.eval()\n",
    "    training_losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for i, (inputs, targets) in enumerate(train_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Student outputs\n",
    "            student_outputs = student_model(inputs)\n",
    "\n",
    "            if epoch < switch_epoch:\n",
    "                # Until switch_epoch use also teacher model for computing loss\n",
    "                with torch.no_grad():\n",
    "                    teacher_outputs = teacher_model(inputs)\n",
    "\n",
    "                loss_soft = F.kl_div(F.log_softmax(student_outputs, dim=1),\n",
    "                                     F.softmax(teacher_outputs, dim=1),\n",
    "                                     reduction='batchmean')\n",
    "            else:\n",
    "                # After a certain number of epochs, the KL divergence only compares the learner with himself\n",
    "                loss_soft = F.kl_div(F.log_softmax(student_outputs, dim=1),\n",
    "                                     F.softmax(student_outputs.detach(), dim=1),\n",
    "                                     reduction='batchmean')\n",
    "\n",
    "            loss_hard = criterion(student_outputs, targets)\n",
    "\n",
    "            # Combined loss\n",
    "            loss = alpha * loss_hard + (1 - alpha) * loss_soft\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if i % 100 == 99:  # Print every 100 mini-batches\n",
    "                #print(f'Epoch [{epoch+1}/{epochs}], Batch [{i+1}/{len(train_loader)}], Loss: {running_loss / 100:.4f}')\n",
    "                training_losses.append(running_loss / 100)\n",
    "                running_loss = 0.0\n",
    "\n",
    "    #plt.plot(training_losses)\n",
    "    #plt.xlabel('Batch (100 intervals)')\n",
    "    #plt.ylabel('Loss')\n",
    "    #plt.title('Training Loss during Distillation')\n",
    "    #plt.show()\n",
    "    return training_losses\n",
    "\n",
    "runs = 15\n",
    "run_accuracies = []\n",
    "for run in range(runs):\n",
    "    print(f\"Starting run {run}...\")\n",
    "    results = {}\n",
    "    switch_epoch_accuracies = []\n",
    "    switch_epochs = epochs+1\n",
    "    for switch_epoch in range(switch_epochs):\n",
    "        print(f\"Training with Distillation with switch_epoch = {switch_epoch}\")\n",
    "    \n",
    "        student_model = StudentModel().to(device)\n",
    "        student_optimizer = optim.Adam(student_model.parameters(), lr=0.001)\n",
    "    \n",
    "        training_losses = train_student(student_model, teacher_model, student_optimizer, switch_epoch)\n",
    "        results[switch_epoch] = training_losses\n",
    "        accuracy = test_model(student_model)\n",
    "        switch_epoch_accuracies.append(accuracy)\n",
    "    run_accuracies.append(switch_epoch_accuracies)\n",
    "    \n",
    "    plt.figure(figsize=(12, 7))\n",
    "    colors = plt.cm.tab20(np.linspace(0, 1, switch_epochs))\n",
    "    for idx, (switch_epoch, losses) in enumerate(results.items()):\n",
    "        plt.plot(losses, label=f'switch_epoch={switch_epoch}', color=colors[idx])\n",
    "    plt.xlabel('Batch (100 intervals)')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Comparison of Different Switch Epoch Values')\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(12, 7))\n",
    "    for idx, (switch_epoch, losses) in enumerate(results.items()):\n",
    "        plt.plot(losses, label=f'switch_epoch={switch_epoch}', color=colors[idx])\n",
    "    plt.xlabel('Batch (100 intervals)')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Comparison of Different switch_epoch Values')\n",
    "    plt.xlim(80, 100)\n",
    "    plt.ylim(0, 0.2)\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(12, 7))\n",
    "    for idx, (switch_epoch, losses) in enumerate(results.items()):\n",
    "        plt.plot(losses, label=f'switch_epoch={switch_epoch}', color=colors[idx])\n",
    "    plt.xlabel('Batch (100 intervals)')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Comparison of Different switch_epoch Values')\n",
    "    plt.xlim(98, 100)\n",
    "    plt.ylim(0.033, 0.062)\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.show()"
   ],
   "id": "b94ca27109aaa803",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "switch_epochs = list(range(epochs+1))\n",
    "plt.figure(figsize=(12, 6))\n",
    "colors = plt.cm.tab20(np.linspace(0, 1, runs))\n",
    "for run in range(runs):\n",
    "    plt.plot(switch_epochs, run_accuracies[run], marker='o', linestyle='dashed', label=f'Run {run + 1}', color=colors[run], alpha=0.7)\n",
    "\n",
    "mean_accuracies = np.mean(run_accuracies, axis=0)\n",
    "\n",
    "plt.plot(switch_epochs, mean_accuracies, marker='o', linestyle='solid', color='black', linewidth=2, label='Mean Accuracy')\n",
    "\n",
    "plt.xlabel('Switch Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Impact of Switch Epoch on Model Accuracy')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), fontsize='small')\n",
    "plt.tight_layout()\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "id": "1c8b7064eed1d3c6",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
