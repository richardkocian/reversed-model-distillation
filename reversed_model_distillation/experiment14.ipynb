{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "epochs = 15\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "class TeacherModel(nn.Module):\n",
    "    def __init__(self, input_size=32):\n",
    "        super(TeacherModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        fc_input_dim = (input_size // 2) * (input_size // 2) * 16\n",
    "        \n",
    "        self.fc1 = nn.Linear(fc_input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 10) # 10 classes for CIFAR-10\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc2(x)\n",
    "\n",
    "def train_teacher(model, optimizer, criterion, alpha=0.6):\n",
    "    model.train()\n",
    "    training_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, (inputs, targets) in enumerate(train_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss_soft = F.kl_div(F.log_softmax(outputs, dim=1),\n",
    "                                     F.softmax(outputs.detach(), dim=1),\n",
    "                                     reduction='batchmean')\n",
    "\n",
    "            loss_hard = criterion(outputs, targets)\n",
    "\n",
    "            # Combined loss\n",
    "            loss = alpha * loss_hard + (1 - alpha) * loss_soft\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            if i % 100 == 99:  # Print every 100 mini-batches\n",
    "                #print(f'Epoch [{epoch+1}/{epochs}], Batch [{i+1}/{len(train_loader)}], Loss: {running_loss / 100:.4f}')\n",
    "                training_losses.append(running_loss / 100)\n",
    "                running_loss = 0.0\n",
    "    # Plotting training loss over time\n",
    "    plt.plot(training_losses)\n",
    "    plt.xlabel('Batch (100 intervals)')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss during training without distillation')\n",
    "    plt.show()\n",
    "    return training_losses\n",
    "\n",
    "def test_model(model):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    print(f'Accuracy: {accuracy}%')\n",
    "    return accuracy\n",
    "\n",
    "teacher_model = TeacherModel().to(device)\n",
    "teacher_optimizer = optim.Adam(teacher_model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"Training Teacher Model...\")\n",
    "train_teacher(teacher_model, teacher_optimizer, criterion)\n",
    "print(\"Teacher model testing:\")\n",
    "test_model(teacher_model)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class StudentModel(nn.Module):\n",
    "    def __init__(self, input_size=32):  # input_size=32 pro CIFAR-10\n",
    "        super(StudentModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "        fc_input_dim = (input_size // 4) * (input_size // 4) * 256\n",
    "\n",
    "        self.fc1 = nn.Linear(fc_input_dim, 512)\n",
    "        self.fc2 = nn.Linear(512, 10)  # 10 classes for CIFAR-10\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc2(x)\n",
    "    \n",
    "def initialize_student_from_teacher(student, teacher, noise_std = 0.01):\n",
    "    \"\"\"\n",
    "    Inicializace první konvoluční vrstvy student modelu pomocí vah teacher modelu.\n",
    "    Teacher má 16 filtrů, student 64 – opakujeme teacherovy váhy 4×.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Původní váhy a bias učitele\n",
    "        teacher_weights = teacher.conv1.weight  # (16, 3, 3, 3)\n",
    "        teacher_bias = teacher.conv1.bias       # (16,)\n",
    "\n",
    "        # 1. Spočítat průměr přes filtry (průměrný filtr)\n",
    "        mean_weight = teacher_weights.mean(dim=0, keepdim=True)  # (1, 3, 3, 3)\n",
    "        mean_bias = teacher_bias.mean().unsqueeze(0)  # (1,)\n",
    "\n",
    "        # 2. Rozkopírovat (opakovat) průměrný filtr a bias 4× (na 64 filtrů)\n",
    "        repeated_weights = mean_weight.repeat(student.conv1.out_channels, 1, 1, 1)  # (64, 3, 3, 3)\n",
    "        repeated_bias = mean_bias.repeat(student.conv1.out_channels)  # (64,)\n",
    "\n",
    "        # 3. Přidat **jiný šum pro každý filtr**\n",
    "        noise_weights = torch.randn_like(repeated_weights) * noise_std\n",
    "        noise_bias = torch.randn_like(repeated_bias) * noise_std\n",
    "\n",
    "        final_weights = repeated_weights + noise_weights\n",
    "        final_bias = repeated_bias + noise_bias\n",
    "\n",
    "        # 4. Zapsat do studenta\n",
    "        student.conv1.weight.copy_(final_weights)\n",
    "        student.conv1.bias.copy_(final_bias)\n",
    "    print(\"Student's conv1 has been initialized from teacher model.\")\n",
    "\n",
    "def train_student(student_model, teacher_model, optimizer, switch_epoch, alpha=0.6):\n",
    "    student_model.train()\n",
    "    teacher_model.eval()\n",
    "    training_losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for i, (inputs, targets) in enumerate(train_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Student outputs\n",
    "            student_outputs = student_model(inputs)\n",
    "\n",
    "            if epoch < switch_epoch:\n",
    "                # Until switch_epoch use also teacher model for computing loss\n",
    "                with torch.no_grad():\n",
    "                    teacher_outputs = teacher_model(inputs)\n",
    "\n",
    "                loss_soft = F.kl_div(F.log_softmax(student_outputs, dim=1),\n",
    "                                     F.softmax(teacher_outputs, dim=1),\n",
    "                                     reduction='batchmean')\n",
    "            else:\n",
    "                # After a certain number of epochs, the KL divergence only compares the learner with himself\n",
    "                loss_soft = F.kl_div(F.log_softmax(student_outputs, dim=1),\n",
    "                                     F.softmax(student_outputs.detach(), dim=1),\n",
    "                                     reduction='batchmean')\n",
    "\n",
    "            loss_hard = criterion(student_outputs, targets)\n",
    "\n",
    "            # Combined loss\n",
    "            loss = alpha * loss_hard + (1 - alpha) * loss_soft\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if i % 100 == 99:  # Print every 100 mini-batches\n",
    "                #print(f'Epoch [{epoch+1}/{epochs}], Batch [{i+1}/{len(train_loader)}], Loss: {running_loss / 100:.4f}')\n",
    "                training_losses.append(running_loss / 100)\n",
    "                running_loss = 0.0\n",
    "\n",
    "    #plt.plot(training_losses)\n",
    "    #plt.xlabel('Batch (100 intervals)')\n",
    "    #plt.ylabel('Loss')\n",
    "    #plt.title('Training Loss during Distillation')\n",
    "    #plt.show()\n",
    "    return training_losses\n",
    "\n",
    "runs = 15\n",
    "run_accuracies = []\n",
    "for run in range(runs):\n",
    "    print(f\"Starting run {run}...\")\n",
    "    results = {}\n",
    "    switch_epoch_accuracies = []\n",
    "    switch_epochs = epochs+1\n",
    "    \n",
    "    print(f\"Training with Distillation with switch_epoch = 0 (baseline training without copying conv1 layer)\")\n",
    "    student_model = StudentModel().to(device)    \n",
    "    student_optimizer = optim.Adam(student_model.parameters(), lr=0.001)\n",
    "\n",
    "    training_losses = train_student(student_model, teacher_model, student_optimizer, 0)\n",
    "    results[-1] = training_losses\n",
    "    accuracy = test_model(student_model)\n",
    "    switch_epoch_accuracies.append(accuracy)\n",
    "    for switch_epoch in range(switch_epochs):\n",
    "        print(f\"Training with Distillation with switch_epoch = {switch_epoch}\")\n",
    "    \n",
    "        student_model = StudentModel().to(device)\n",
    "        \n",
    "        initialize_student_from_teacher(student_model, teacher_model)\n",
    "        \n",
    "        student_optimizer = optim.Adam(student_model.parameters(), lr=0.001)\n",
    "    \n",
    "        training_losses = train_student(student_model, teacher_model, student_optimizer, switch_epoch)\n",
    "        results[switch_epoch] = training_losses\n",
    "        accuracy = test_model(student_model)\n",
    "        switch_epoch_accuracies.append(accuracy)\n",
    "    run_accuracies.append(switch_epoch_accuracies)\n",
    "    \n",
    "    plt.figure(figsize=(12, 7))\n",
    "    colors = plt.cm.tab20(np.linspace(0, 1, switch_epochs+1))\n",
    "    for idx, (switch_epoch, losses) in enumerate(results.items()):\n",
    "        plt.plot(losses, label=f'switch_epoch={switch_epoch}', color=colors[idx])\n",
    "    plt.xlabel('Batch (100 intervals)')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Comparison of Different Switch Epoch Values')\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(12, 7))\n",
    "    for idx, (switch_epoch, losses) in enumerate(results.items()):\n",
    "        plt.plot(losses, label=f'switch_epoch={switch_epoch}', color=colors[idx])\n",
    "    plt.xlabel('Batch (100 intervals)')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Comparison of Different Switch Epoch Values')\n",
    "    plt.xlim(80, 100)\n",
    "    plt.ylim(0, 0.2)\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(12, 7))\n",
    "    for idx, (switch_epoch, losses) in enumerate(results.items()):\n",
    "        plt.plot(losses, label=f'switch_epoch={switch_epoch}', color=colors[idx])\n",
    "    plt.xlabel('Batch (100 intervals)')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Comparison of Different Switch Epoch Values')\n",
    "    plt.xlim(98, 100)\n",
    "    plt.ylim(0.033, 0.062)\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.show()"
   ],
   "id": "b94ca27109aaa803",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "switch_epochs = [-1] + list(range(epochs+1))\n",
    "plt.figure(figsize=(12, 6))\n",
    "colors = plt.cm.tab20(np.linspace(0, 1, runs))\n",
    "for run in range(runs):\n",
    "    plt.plot(switch_epochs, run_accuracies[run], marker='o', linestyle='dashed', label=f'Run {run + 1}', color=colors[run], alpha=0.7)\n",
    "\n",
    "mean_accuracies = np.mean(run_accuracies, axis=0)\n",
    "\n",
    "plt.plot(switch_epochs, mean_accuracies, marker='o', linestyle='solid', color='black', linewidth=2, label='Mean Accuracy')\n",
    "\n",
    "plt.xlabel('Switch Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Impact of Switch Epoch on Model Accuracy')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), fontsize='small')\n",
    "plt.tight_layout()\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "id": "1c8b7064eed1d3c6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "switch_epochs = [-1] + list(range(epochs+1))\n",
    "plt.figure(figsize=(13, 6))\n",
    "colors = plt.cm.tab20(np.linspace(0, 1, runs))\n",
    "for run in range(runs):\n",
    "    plt.plot(switch_epochs, run_accuracies[run], marker='o', linestyle='dashed', label=f'Run {run + 1}', color=colors[run], alpha=0.7)\n",
    "\n",
    "mean_accuracies = np.mean(run_accuracies, axis=0)\n",
    "\n",
    "plt.plot(switch_epochs, mean_accuracies, marker='o', linestyle='solid', color='black', linewidth=2, label='Mean Accuracy')\n",
    "plt.axhline(y=mean_accuracies[0], color='red', linestyle='dotted', linewidth=2, label='Baseline (Switch Epoch 0\\n without copying teacher layer)')\n",
    "\n",
    "plt.xlabel('Switch Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Impact of Switch Epoch on Model Accuracy')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), fontsize='small')\n",
    "plt.tight_layout()\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "id": "1cc121813c0adb15",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "save_dir = \"experiment7_run_accuracies\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "for i, run in enumerate(run_accuracies):\n",
    "    filename = os.path.join(save_dir, f\"run_{i + 1}.txt\")\n",
    "    np.savetxt(filename, np.array(run), fmt=\"%.4f\")\n",
    "\n",
    "np.savetxt(f\"{save_dir}/mean_accuracies.txt\", mean_accuracies, fmt=\"%.4f\")\n",
    "\n",
    "torch.save(teacher_model, \"teacher_model_experiment7.pth\")"
   ],
   "id": "66c03504d942ec26",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
