{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "epochs = 15\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "class TeacherModel(nn.Module):\n",
    "    def __init__(self, input_size=32):\n",
    "        super(TeacherModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        fc_input_dim = (input_size // 2) * (input_size // 2) * 16\n",
    "        \n",
    "        self.fc1 = nn.Linear(fc_input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 10) # 10 classes for CIFAR-10\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc2(x)\n",
    "    \n",
    "def test_model(model):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    print(f'Accuracy: {accuracy}%')\n",
    "    return accuracy\n",
    "\n",
    "teacher_model = torch.load('teacher_model_hard_loss.pth', weights_only=False)\n",
    "print(\"Teacher model testing:\")\n",
    "test_model(teacher_model)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "class StudentModel(nn.Module):\n",
    "    def __init__(self, input_size=32):  # input_size=32 pro CIFAR-10\n",
    "        super(StudentModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "        fc_input_dim = (input_size // 4) * (input_size // 4) * 256\n",
    "\n",
    "        self.fc1 = nn.Linear(fc_input_dim, 512)\n",
    "        self.fc2 = nn.Linear(512, 10)  # 10 classes for CIFAR-10\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc2(x)\n",
    "    \n",
    "def initialize_student_from_teacher(student, teacher):\n",
    "    \"\"\"\n",
    "    Inicializace první konvoluční vrstvy student modelu pomocí normalizovaných vah teacher modelu.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # teacher.conv1.weight: tvar (16, 3, 3, 3)\n",
    "        # student.conv1.weight: tvar (64, 3, 3, 3)\n",
    "        \n",
    "        teacher_weights = teacher.conv1.weight  # Váhy učitele (16, 3, 3, 3)\n",
    "        teacher_bias = teacher.conv1.bias       # Bias učitele (16,)\n",
    "\n",
    "        # Normalizace vah učitele (z-score)\n",
    "        mean = teacher_weights.mean()\n",
    "        std = teacher_weights.std()\n",
    "        normalized_weights = (teacher_weights - mean) / (std + 1e-5)  # +1e-5 pro numerickou stabilitu\n",
    "        \n",
    "        # Normalizace biasu\n",
    "        mean_bias = teacher_bias.mean()\n",
    "        std_bias = teacher_bias.std()\n",
    "        normalized_bias = (teacher_bias - mean_bias) / (std_bias + 1e-5)\n",
    "        \n",
    "        # Kopírování do studenta\n",
    "        factor = student.conv1.out_channels // teacher.conv1.out_channels  # 64 // 16 = 4\n",
    "        student.conv1.weight.copy_(normalized_weights.repeat(factor, 1, 1, 1))\n",
    "        student.conv1.bias.copy_(normalized_bias.repeat(factor))\n",
    "\n",
    "    print(\"Student's conv1 has been initialized from normalized teacher model.\")\n",
    "\n",
    "def train_student(student_model, teacher_model, optimizer, switch_epoch, alpha=0.6):\n",
    "    student_model.train()\n",
    "    teacher_model.eval()\n",
    "    training_losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for i, (inputs, targets) in enumerate(train_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Student outputs\n",
    "            student_outputs = student_model(inputs)\n",
    "\n",
    "            if epoch < switch_epoch:\n",
    "                # Until switch_epoch use also teacher model for computing loss\n",
    "                with torch.no_grad():\n",
    "                    teacher_outputs = teacher_model(inputs)\n",
    "\n",
    "                loss_soft = F.kl_div(F.log_softmax(student_outputs, dim=1),\n",
    "                                     F.softmax(teacher_outputs, dim=1),\n",
    "                                     reduction='batchmean')\n",
    "            else:\n",
    "                # After a certain number of epochs, the KL divergence only compares the learner with himself\n",
    "                loss_soft = F.kl_div(F.log_softmax(student_outputs, dim=1),\n",
    "                                     F.softmax(student_outputs.detach(), dim=1),\n",
    "                                     reduction='batchmean')\n",
    "\n",
    "            loss_hard = criterion(student_outputs, targets)\n",
    "\n",
    "            # Combined loss\n",
    "            loss = alpha * loss_hard + (1 - alpha) * loss_soft\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if i % 100 == 99:  # Print every 100 mini-batches\n",
    "                #print(f'Epoch [{epoch+1}/{epochs}], Batch [{i+1}/{len(train_loader)}], Loss: {running_loss / 100:.4f}')\n",
    "                training_losses.append(running_loss / 100)\n",
    "                running_loss = 0.0\n",
    "\n",
    "    #plt.plot(training_losses)\n",
    "    #plt.xlabel('Batch (100 intervals)')\n",
    "    #plt.ylabel('Loss')\n",
    "    #plt.title('Training Loss during Distillation')\n",
    "    #plt.show()\n",
    "    return training_losses\n",
    "\n",
    "runs = 15\n",
    "run_accuracies = []\n",
    "for run in range(runs):\n",
    "    print(f\"Starting run {run}...\")\n",
    "    results = {}\n",
    "    switch_epoch_accuracies = []\n",
    "    switch_epochs = epochs+1\n",
    "    \n",
    "    print(f\"Training with Distillation with switch_epoch = 0 (baseline training without copying conv1 layer)\")\n",
    "    student_model = StudentModel().to(device)    \n",
    "    student_optimizer = optim.Adam(student_model.parameters(), lr=0.001)\n",
    "\n",
    "    training_losses = train_student(student_model, teacher_model, student_optimizer, 0)\n",
    "    results[-1] = training_losses\n",
    "    accuracy = test_model(student_model)\n",
    "    switch_epoch_accuracies.append(accuracy)\n",
    "    for switch_epoch in range(switch_epochs):\n",
    "        print(f\"Training with Distillation with switch_epoch = {switch_epoch}\")\n",
    "    \n",
    "        student_model = StudentModel().to(device)\n",
    "        \n",
    "        initialize_student_from_teacher(student_model, teacher_model)\n",
    "        \n",
    "        student_optimizer = optim.Adam(student_model.parameters(), lr=0.001)\n",
    "    \n",
    "        training_losses = train_student(student_model, teacher_model, student_optimizer, switch_epoch)\n",
    "        results[switch_epoch] = training_losses\n",
    "        accuracy = test_model(student_model)\n",
    "        switch_epoch_accuracies.append(accuracy)\n",
    "    run_accuracies.append(switch_epoch_accuracies)\n",
    "    \n",
    "    plt.figure(figsize=(12, 7))\n",
    "    colors = plt.cm.tab20(np.linspace(0, 1, switch_epochs+1))\n",
    "    for idx, (switch_epoch, losses) in enumerate(results.items()):\n",
    "        plt.plot(losses, label=f'switch_epoch={switch_epoch}', color=colors[idx])\n",
    "    plt.xlabel('Batch (100 intervals)')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Comparison of Different Switch Epoch Values')\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(12, 7))\n",
    "    for idx, (switch_epoch, losses) in enumerate(results.items()):\n",
    "        plt.plot(losses, label=f'switch_epoch={switch_epoch}', color=colors[idx])\n",
    "    plt.xlabel('Batch (100 intervals)')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Comparison of Different Switch Epoch Values')\n",
    "    plt.xlim(80, 100)\n",
    "    plt.ylim(0, 0.2)\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(12, 7))\n",
    "    for idx, (switch_epoch, losses) in enumerate(results.items()):\n",
    "        plt.plot(losses, label=f'switch_epoch={switch_epoch}', color=colors[idx])\n",
    "    plt.xlabel('Batch (100 intervals)')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Comparison of Different Switch Epoch Values')\n",
    "    plt.xlim(98, 100)\n",
    "    plt.ylim(0.033, 0.062)\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.show()"
   ],
   "id": "b94ca27109aaa803",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "switch_epochs = [-1] + list(range(epochs+1))\n",
    "plt.figure(figsize=(12, 6))\n",
    "colors = plt.cm.tab20(np.linspace(0, 1, runs))\n",
    "for run in range(runs):\n",
    "    plt.plot(switch_epochs, run_accuracies[run], marker='o', linestyle='dashed', label=f'Run {run + 1}', color=colors[run], alpha=0.7)\n",
    "\n",
    "mean_accuracies = np.mean(run_accuracies, axis=0)\n",
    "\n",
    "plt.plot(switch_epochs, mean_accuracies, marker='o', linestyle='solid', color='black', linewidth=2, label='Mean Accuracy')\n",
    "\n",
    "plt.xlabel('Switch Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Impact of Switch Epoch on Model Accuracy')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), fontsize='small')\n",
    "plt.tight_layout()\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "id": "1c8b7064eed1d3c6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "switch_epochs = [-1] + list(range(epochs+1))\n",
    "plt.figure(figsize=(13, 6))\n",
    "colors = plt.cm.tab20(np.linspace(0, 1, runs))\n",
    "for run in range(runs):\n",
    "    plt.plot(switch_epochs, run_accuracies[run], marker='o', linestyle='dashed', label=f'Run {run + 1}', color=colors[run], alpha=0.7)\n",
    "\n",
    "mean_accuracies = np.mean(run_accuracies, axis=0)\n",
    "\n",
    "plt.plot(switch_epochs, mean_accuracies, marker='o', linestyle='solid', color='black', linewidth=2, label='Mean Accuracy')\n",
    "plt.axhline(y=mean_accuracies[0], color='red', linestyle='dotted', linewidth=2, label='Baseline (Switch Epoch 0\\n without copying teacher layer)')\n",
    "\n",
    "plt.xlabel('Switch Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Impact of Switch Epoch on Model Accuracy')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), fontsize='small')\n",
    "plt.tight_layout()\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "id": "1cc121813c0adb15",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "save_dir = \"experiment15_run_accuracies\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "for i, run in enumerate(run_accuracies):\n",
    "    filename = os.path.join(save_dir, f\"run_{i + 1}.txt\")\n",
    "    np.savetxt(filename, np.array(run), fmt=\"%.4f\")\n",
    "\n",
    "np.savetxt(f\"{save_dir}/mean_accuracies.txt\", mean_accuracies, fmt=\"%.4f\")"
   ],
   "id": "66c03504d942ec26",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
